{"start_training": {"mode": "train", # "train" or "eval"
                    "eval": {"choices": ["if_linear"], # ["if_embeddings",  "if_knn", "if_linear", "if_throughput"]
                             "epoch": 0,
                             "linear":{"batch_size": 1024,
                                       "num_epochs": 100,
                                       "n_last_blocks": 4,
                                       "avgpool_patchtokens": False,
                                       "lr": 0.001,
                                       "momentum": 0.9,
                                       "wd": 0,
                                       "val_freq": 1,
                                       "restore_epoch": 0,
                                      }
                            }
                            }, # Note that if if_throughput, there should be just 1 gpu visible and the batch size should be 128
"save_params": { "output_dir": "/storage/yue/dino_models/ViT-S-16-CIFAR100/",
                 "restore_epoch": 0,
                 "saveckp_freq": 1,
                 "tb_logoriginal": True, # Whether to log the embeddings and knn results before the training starts
                 "tb_freq": 101, # Frequency to save embeddings and knn results
                 "nb_knn": [20],
                 "temp_knn": 0.07
               },
"dataset_params": {"data_folder": "/storage/yue/data/",
                   "specification": { "dataset_name": "CIFAR100",  # "ImageNet", "IMAGENETTE", "CIFAR10", "CIFAR100"
                                      "ImageNet":{ "num_labels": 1000,
                                                   "knn_use_cuda": False,
                                                   "label_mapping_path": 'data_label_mapping/ImageNet.json'},
                                      "IMAGENETTE":{ "num_labels": 10,
                                                     "knn_use_cuda": False,
                                                     "label_mapping_path": 'data_label_mapping/ImageNet.json'},
                                      "CIFAR10": { "num_labels": 10,
                                                   "knn_use_cuda": False,
                                                   "label_mapping_path": 'data_label_mapping/CIFAR10.json' },
                                      "CIFAR100": { "num_labels": 100,
                                                    "knn_use_cuda": False,
                                                    "label_mapping_path": 'data_label_mapping/CIFAR100.json' },

                    }
 },
"dataloader_params": {"trainloader":{"batch_size": 32,
                                     "batch_size_for_scheduler": 32,
                                     "num_workers": 10, # 'Number of data loading workers per GPU.'
                                     "pin_memory": True,
                                     "drop_last": True},
                      "valloader": { "batch_size": 32,
                                     "num_workers": 10, # 'Number of data loading workers per GPU.'
                                     "pin_memory": False,
                                     "drop_last": False}
                     },
"augmentation_params": {"global_crops_scale": [0.25, 1.0],
                        "local_crops_scale": [0.05, 0.25],
                        "local_crops_number": 10,
                        "full_size": 256,
                        "global_size": 224,
                        "local_size": 96,
                       },
"model_params": {"backbone_option":  "vit_small", # 'vit_tiny', 'vit_small', 'vit_base', 'resnet50', 'deit_tiny', 'deit_small'
                 "patch_size": 16,
                 "drop_path_rate": 0.1,
                 "out_dim": 65536, # "Dimensionality of the DINO head output. For complex and large datasets large values (like 65k) work well."
                 "use_bn_in_head": False, # "Whether to use batch normalizations in projection head (Default: False)"
                 "norm_last_layer": False, # "Whether or not to weight normalize the last layer of the DINO head.
                 # Not normalizing leads to better performance but can make the training unstable.
                 # In our experiments, we typically set this paramater to False with vit_small and True with vit_base."
                },
"training_params": {"num_epochs": 100,
                    "num_epochs_for_scheduler": 300,
                    "warmup_teacher_temp": 0.04,
                    "teacher_temp": 0.07, # """Final value (after linear warmup)
#                                         # of the teacher temperature. For most experiments, anything above 0.07 is unstable. We recommend
#                                         # starting with the default value of 0.04 and increase this slightly if needed."""
                    "warmup_teacher_temp_epochs": 30,
                    "student_temp": 0.1,
                    "center_momentum": 0.9,
                    "optimizer": {"name": "adamw", # adamw, sgd
                                  "sgd": { "lr": 0, "momentum": 0.9}},
                    "lr":{"base_lr": 0.0005,
                          "final_lr": 0.00001,
                          "warmup_epochs": 10,
                          "start_warmup_lr": 0},
                    "wd":{"base_wd": 0.04,
                          "final_wd": 0.4},
                    "momentum": {"base_momentum_teacher": 0.996,
                                 "final_momentum_teacher": 1},
                    "clip_grad": 0,
                    "freeze_last_layer": 1,
                    },
"system_params": {"num_gpus": 2,
                  "gpu_ids": "0,1",
                  "random_seed": 0,
                  "use_fp16": False # """Whether or not to use half precision for training. Improves training time and memory requirements,
                                    #  but can provoke instability and slight decay of performance. We recommend disabling
                                    #  mixed precision if the loss is unstable, if reducing the patch size or if training with bigger ViTs.""")
                 },
}